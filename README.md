# README

## Overview

This project is a specialized web scraping and data processing pipeline designed to extract, clean, and structure information related to upcoming election dates, filing start and end dates, and office positions in the USA government. The pipeline consists of several stages, including web scraping, data filtering, and data processing, ultimately storing the structured data in JSON or CSV format.

## Project Structure

- **web_crawler_and_scraper**: Contains scripts for web scraping and initial data cleaning.
- **url_filter_and_extraction**: Contains scripts for scoring and filtering data, as well as extracting relevant information.
- **data_processor**: Contains scripts for processing extracted data and storing it in structured formats.

## System Flow

1. **Get URLs**:

   - **Input**: Prompt and keywords.
   - **Process**: Use Google programmable search engine's Custom Search JSON API and Tavily's Search API to get the URLs.
   - **Output**: A list of URLs.

2. **Web Scraping**:

   - **Input**: List of URLs.
   - **Process**: Use `tavily` to extract descriptions from URLs. Send descriptions to OpenAI to determine relevance.
   - **Output**: Relevant URLs for further processing.

3. **Data Extraction**:

   - **Input**: Relevant URLs.
   - **Process**: Extract roles and positions using OpenAI's LLM. Validate and format positions to ensure they follow the format "role: [local government position] of region: [county/municipality/township]".
   - **Output**: Validated and formatted positions.

4. **Data Processing**:

   - **Input**: Validated positions.
   - **Process**: Use LLM to extract structured data for each position, including details like position title, description, election dates, and filing windows.
   - **Output**: Structured data in JSON format.

5. **Storage**:
   - **Input**: Structured data.
   - **Process**: Store data in JSON or CSV format, organized by role or region.
   - **Output**: Data files ready for analysis or reporting.

## Usage

### Step 1: Get URLs

- The `get_urls.py` script is used to get the URLs of the pages to scrape.
- The script takesa prompt and optionally keywords as input as uses Google programmable search engline's Custom Search JSON API and Tavily's Search API to get the URLs.
- An initial filteration is done by using descriptions generated by the `generate_description.py` script and using LLM to filter out URLs that are not relevant. The filtered URLs are stored in `./urls/initial_filtered_urls.txt`.

### Step 2: Web Scraping

To start the web scraping process, run the following command:

```bash
$ python ./web_crawler_and_scraper/scrape_urls.py
```

- This script will scrape the filtered URLs and store the raw HTML content in the `scraped_files` directory.
- The cleaned and formatted files will be stored in the `formatted_files` directory inside `url_filter_and_extraction`.
- You can set the number of pages to scrape by changing the `pages_to_scrape` variable in the `scrape_urls.py` script.

### Step 3: Scoring and Filtering

Run the `score_and_filter.py` script in the `url_filter_and_extraction` directory:

```bash
$ python ./url_filter_and_extraction/score_and_filter.py
```

- This script scores each file based on its content. Files with a score greater than a cutoff score are moved to the `filtered` directory.
- You can set the cutoff score by changing the `CUTOFF_SCORE` variable in the `score_and_filter.py` script.

### Step 4: Data Extraction

Run the `extraction.py` script in the `url_filter_and_extraction` directory:

```bash
$ python ./url_filter_and_extraction/extraction.py
```

- This script extracts important information from the filtered files and stores them in the `shared_data/unstructured_data` directory, along with their URLs.
- You can adjust the prompt used for extraction by changing the `generate_extraction_prompt` function in the `extraction.py` script to better suit your needs.

### Step 5: Data Processing

Finally, process the extracted data using the `processor.py` script in the `data_processor` directory:

```bash
$ python ./data_processor/processor.py
```

- The `use_llm_for_extraction` function in the `llmPrompts.py` script uses LLM to extract the roles from the filtered files and uses the `use_llm_for_position_data` function to fetch and save data for each role.
- You can adjust the prompt used for extraction by changing the system prompts used in the `llmPrompts.py` file in the `data_processor` directory to better suit your needs.
- The unstructured data is validated using a pydantic schema, and stored in a structured format either in JSON or CSV in the `shared_data/structured_data_*` directories.
- If using JSON, files belonging to the same role are stored in the same folder.
- If using CSV, data corresponding to the same role is stored in the same CSV file.

## Requirements

Ensure you have all necessary dependencies installed. You can manage Python packages using Poetry as specified in your environment setup.

## Diagrams included

To better understand the system, consider the following diagrams:

1. **System Architecture Diagram**: Illustrates the components of the system and their interactions, including the flow of data from web scraping to storage.

2. **Data Flow Diagram**: Shows the flow of data through the system, highlighting key processes like validation and extraction.

3. **Sequence Diagram**: Details the sequence of operations for a single URL, from initial scraping to final data storage.

## Notes

- Edit the `.env.sample` file to include your OpenAI API key and save it as `.env`.
- The project is designed to run on Windows using Git Bash as the terminal. Adjust paths and commands if using a different setup.
- The files in the `shared_data` directory are not tracked by git to avoid large file storage issues.
